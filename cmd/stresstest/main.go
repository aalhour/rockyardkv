// Full stress test for RockyardKV
//
// This tool performs comprehensive stress testing using an expected state oracle
// to verify database correctness.
//
// KEY DESIGN FEATURES (matching C++ RocksDB db_stress):
//   - Per-key locking: Each write operation acquires a lock for the key before
//     modifying the expected state, ensuring atomicity between DB ops and oracle.
//   - Pending state tracking: Uses PendingExpectedValue with Commit/Rollback semantics.
//   - Pre/Post read verification: For reads, captures expected state before and after
//     the operation to handle concurrent modifications gracefully.
//
// Features:
// - Random puts, gets, deletes
// - Batch writes
//
// Reference: RocksDB v10.7.5
//   - db_stress_tool/db_stress.cc
//   - db_stress_tool/db_stress_driver.cc
//
// - Iterator verification
// - Snapshot reads
// - Range scans
// - Database reopening (persistence checks)
// - Concurrent access with per-key locking
// - Compaction triggering
//
// Usage: go run ./cmd/stresstest [flags]
package main

import (
	"bytes"
	"encoding/binary"
	"errors"
	"flag"
	"fmt"
	"math/rand"
	"os"
	"path/filepath"
	"runtime"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/aalhour/rockyardkv/db"
	"github.com/aalhour/rockyardkv/internal/batch"
	"github.com/aalhour/rockyardkv/internal/testutil"
)

var (
	// Test configuration
	duration             = flag.Duration("duration", 60*time.Second, "Test duration")
	numKeys              = flag.Int64("keys", 10000, "Number of keys in the key space")
	valueSize            = flag.Int("value-size", 100, "Size of each value in bytes")
	numThreads           = flag.Int("threads", 100, "Number of concurrent threads")
	reopenPeriod         = flag.Duration("reopen", 10*time.Second, "Period between database reopens (0 to disable)")
	flushPeriod          = flag.Duration("flush", 5*time.Second, "Period between flushes (0 to disable)")
	compactEvery         = flag.Int("compact-every", 0, "Compact after N operations (0 to disable)")
	dbPath               = flag.String("db", "", "Database path (default: temp directory)")
	keepDB               = flag.Bool("keep", false, "Keep database after test")
	cleanup              = flag.Bool("cleanup", false, "Clean up old test directories before running")
	verbose              = flag.Bool("v", false, "Verbose output")
	seed                 = flag.Int64("seed", 0, "Random seed (0 for time-based)")
	expectedState        = flag.String("expected-state", "", "Path to expected state file (for persistence across crashes)")
	saveExpected         = flag.Bool("save-expected", false, "Save expected state after test")
	saveExpectedInterval = flag.Duration("save-expected-interval", 1*time.Second, "Interval to persist expected state during the run (0 to disable)")
	verifyOnly           = flag.Bool("verify-only", false, "Verify database state using expected state file, without running operations")
	allowDBAhead         = flag.Bool("allow-db-ahead", false, "Allow DB to have more data than expected state (for crash testing with race conditions)")

	// Operation weights (sum to 100)
	putWeight            = flag.Int("put", 30, "Put operation weight")
	getWeight            = flag.Int("get", 25, "Get operation weight")
	deleteWeight         = flag.Int("delete", 10, "Delete operation weight")
	batchWeight          = flag.Int("batch", 10, "Batch write weight")
	iterWeight           = flag.Int("iter", 5, "Iterator scan weight")
	snapshotWeight       = flag.Int("snapshot", 5, "Snapshot read weight")
	rangeDelWeight       = flag.Int("range-delete", 5, "Range deletion weight")
	mergeWeight          = flag.Int("merge", 5, "Merge operation weight")
	ingestWeight         = flag.Int("ingest", 0, "SST ingestion weight (default 0 - has subtle expected state tracking issues in concurrent scenarios)")
	transactionWeight    = flag.Int("txn", 5, "Transaction weight")
	compactWeight        = flag.Int("compact", 2, "Compaction trigger weight")
	snapshotVerifyWeight = flag.Int("snapshot-verify", 3, "Snapshot isolation verification weight")
	cfWeight             = flag.Int("cf", 5, "Column family operations weight")

	// Verification options
	verifyEvery   = flag.Int("verify-every", 10000, "Verify random keys every N operations")
	verifyPercent = flag.Int("verify-percent", 10, "Percent of keys to verify in spot checks")

	// Locking configuration (matching RocksDB's log2_keys_per_lock)
	log2KeysPerLock = flag.Uint("log2-keys-per-lock", 2, "Log2 of number of keys per lock (default: 4 keys per lock)")

	// Database options (for randomization)
	compressionType   = flag.String("compression", "none", "Compression type: none, snappy, zlib, random")
	checksumType      = flag.String("checksum", "crc32c", "Checksum type: crc32c, xxh3, random")
	disableWAL        = flag.Bool("disable-wal", false, "Disable write-ahead log")
	syncWrites        = flag.Bool("sync", false, "Sync writes to disk")
	blockSize         = flag.Int("block-size", 4096, "SST block size in bytes")
	writeBufferSize   = flag.Int("write-buffer-size", 4*1024*1024, "Write buffer (memtable) size in bytes")
	_                 = flag.Int("max-open-files", -1, "Max open files (-1 for unlimited)") // Reserved for future use
	bloomBits         = flag.Int("bloom-bits", 10, "Bloom filter bits per key (0 to disable)")
	numColumnFamilies = flag.Int("column-families", 1, "Number of column families")

	// Randomization flags
	randomizeParams = flag.Bool("randomize", false, "Randomize database parameters")
)

// errStopped is returned when an operation is cancelled due to stop signal.
var errStopped = fmt.Errorf("stopped")

// Stats tracks operation counts
type Stats struct {
	puts             atomic.Uint64
	gets             atomic.Uint64
	deletes          atomic.Uint64
	batches          atomic.Uint64
	iterScans        atomic.Uint64
	snapshotReads    atomic.Uint64
	rangeDeletes     atomic.Uint64
	merges           atomic.Uint64
	ingests          atomic.Uint64
	compactions      atomic.Uint64
	transactions     atomic.Uint64
	txnCommits       atomic.Uint64
	txnRollbacks     atomic.Uint64
	snapshotVerifies atomic.Uint64
	iterVerifies     atomic.Uint64
	cfOps            atomic.Uint64
	errors           atomic.Uint64
	verifyFail       atomic.Uint64
	reopens          atomic.Uint64
	flushes          atomic.Uint64
	spotVerifyPass   atomic.Uint64
}

// stressWriteOpts contains the write options used by all stress test operations.
// Initialized in main() based on -sync and -disable-wal flags.
var stressWriteOpts *db.WriteOptions

func main() {
	flag.Parse()

	if *seed == 0 {
		*seed = time.Now().UnixNano()
	}

	rand.Seed(*seed)

	// Apply randomization if requested
	if *randomizeParams {
		randomizeAllParameters()
	}

	// Create write options based on flags
	// This must happen AFTER randomization to pick up any randomized values
	stressWriteOpts = db.DefaultWriteOptions()
	stressWriteOpts.Sync = *syncWrites
	stressWriteOpts.DisableWAL = *disableWAL

	printBanner()

	// Setup database path
	testDir := setupDBPath()

	// Create or load expected state oracle with per-key locking
	var expState *testutil.ExpectedStateV2
	if *expectedState != "" {
		// Try to load from file
		loaded, err := testutil.LoadExpectedStateV2FromFile(*expectedState)
		if err == nil {
			expState = loaded
			if *verbose {
				fmt.Printf("ğŸ“‚ Loaded expected state from %s (seqno: %d)\n", *expectedState, expState.GetPersistedSeqno())
			}
		} else if *verbose {
			fmt.Printf("ğŸ“‚ Could not load expected state from %s: %v (creating new)\n", *expectedState, err)
		}
	}
	if expState == nil {
		expState = testutil.NewExpectedStateV2(*numKeys, 1, uint32(*log2KeysPerLock))
	}

	// Periodically persist expected state for crash testing.
	// This makes -save-expected meaningful under SIGKILL.
	stopSave := make(chan struct{})
	if *expectedState != "" && *saveExpected && *saveExpectedInterval > 0 {
		// Save immediately so crash tests have an initial state file
		// even if killed before the first tick.
		_ = expState.SaveToFile(*expectedState)

		go func() {
			t := time.NewTicker(*saveExpectedInterval)
			defer t.Stop()
			for {
				select {
				case <-t.C:
					_ = expState.SaveToFile(*expectedState)
				case <-stopSave:
					return
				}
			}
		}()
	}
	defer close(stopSave)

	// Run stress test
	stats := &Stats{}
	if *verifyOnly {
		if *expectedState == "" {
			fmt.Println("\nâŒ STRESS TEST FAILED: -verify-only requires -expected-state")
			os.Exit(1)
		}
		database, _, err := openDB(testDir)
		if err != nil {
			fmt.Printf("\nâŒ STRESS TEST FAILED: open failed: %v\n", err)
			os.Exit(1)
		}
		defer database.Close()

		fmt.Println("\nğŸ” Running final verification...")
		if err := verifyAll(database, expState, stats); err != nil {
			fmt.Printf("\nâŒ STRESS TEST FAILED: final verification failed: %v\n", err)
			os.Exit(1)
		}
		fmt.Println("âœ… VERIFICATION PASSED")
		return
	}
	if err := runStressTest(testDir, expState, stats); err != nil {
		fmt.Printf("\nâŒ STRESS TEST FAILED: %v\n", err)
		os.Exit(1)
	}

	// Save expected state if requested
	if *expectedState != "" && *saveExpected {
		if err := expState.SaveToFile(*expectedState); err != nil {
			fmt.Printf("âš ï¸  Failed to save expected state: %v\n", err)
		} else if *verbose {
			fmt.Printf("ğŸ’¾ Saved expected state to %s\n", *expectedState)
		}
	}

	// Print final stats
	printStats(stats)

	// Check for failures
	if stats.errors.Load() > 0 {
		fmt.Println("âŒ STRESS TEST FAILED")
		os.Exit(1)
	}

	// Verification failures should now be rare with per-key locking
	if stats.verifyFail.Load() > 0 {
		fmt.Printf("âš ï¸  %d verification failures\n", stats.verifyFail.Load())
		fmt.Println("âŒ STRESS TEST FAILED")
		os.Exit(1)
	}

	fmt.Println("âœ… STRESS TEST PASSED")

	if *keepDB {
		fmt.Printf("\nğŸ“ Database kept at: %s\n", testDir)
	} else if *dbPath == "" {
		// Clean up temp directory
		os.RemoveAll(testDir)
	}
}

func printBanner() {
	// Helper to print a line with proper right border alignment (70 chars inner width)
	line := func(content string) {
		fmt.Printf("â•‘ %-63s â•‘\n", content)
	}

	fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
	fmt.Println("â•‘              RockyardKV Full Stress Test (v2)                â•‘")
	fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
	line(fmt.Sprintf("Duration: %-10s Keys: %-10d Threads: %-6d", *duration, *numKeys, *numThreads))
	line(fmt.Sprintf("Seed: %-20d", *seed))
	line(fmt.Sprintf("Value Size: %-6d bytes  Keys/Lock: %-4d", *valueSize, 1<<*log2KeysPerLock))
	fmt.Println("â• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•£")
	line(fmt.Sprintf("Weights: put=%d get=%d del=%d batch=%d iter=%d snap=%d",
		*putWeight, *getWeight, *deleteWeight, *batchWeight, *iterWeight, *snapshotWeight))
	line(fmt.Sprintf("         range-del=%d merge=%d ingest=%d txn=%d compact=%d ",
		*rangeDelWeight, *mergeWeight, *ingestWeight, *transactionWeight, *compactWeight))
	line(fmt.Sprintf("         snap-verify=%d cf=%d", *snapshotVerifyWeight, *cfWeight))
	fmt.Println("â• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•£")
	line(fmt.Sprintf("Compression: %-8s  Checksum: %-8s  Bloom: %-4d bits",
		*compressionType, *checksumType, *bloomBits))
	line(fmt.Sprintf("Block Size: %-8d  Write Buffer: %-10d  CFs: %-4d",
		*blockSize, *writeBufferSize, *numColumnFamilies))
	walStatus := "enabled"
	if *disableWAL {
		walStatus = "disabled"
	}
	syncStatus := "off"
	if *syncWrites {
		syncStatus = "on"
	}
	line(fmt.Sprintf("WAL: %-8s  Sync: %-4s  Randomized: %-5v", walStatus, syncStatus, *randomizeParams))
	fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println()
}

// randomizeAllParameters randomly selects database configuration parameters.
// This is inspired by RocksDB's db_crashtest.py parameter randomization.
func randomizeAllParameters() {
	// Compression type
	compressionTypes := []string{"none", "snappy", "zlib"}
	*compressionType = compressionTypes[rand.Intn(len(compressionTypes))]

	// Checksum type
	checksumTypes := []string{"crc32c", "xxh3"}
	*checksumType = checksumTypes[rand.Intn(len(checksumTypes))]

	// Block size (4KB to 64KB)
	blockSizes := []int{4096, 8192, 16384, 32768, 65536}
	*blockSize = blockSizes[rand.Intn(len(blockSizes))]

	// Write buffer size (1MB to 16MB)
	writeBufferSizes := []int{1 * 1024 * 1024, 2 * 1024 * 1024, 4 * 1024 * 1024, 8 * 1024 * 1024, 16 * 1024 * 1024}
	*writeBufferSize = writeBufferSizes[rand.Intn(len(writeBufferSizes))]

	// Bloom filter bits (0 to 20)
	*bloomBits = rand.Intn(21)

	// WAL and sync (mostly enabled)
	*disableWAL = rand.Intn(10) == 0 // 10% chance of disabling WAL
	*syncWrites = rand.Intn(5) == 0  // 20% chance of sync writes

	// Operation weights (randomize while keeping sum around 100)
	total := 100
	*putWeight = 20 + rand.Intn(40) // 20-59
	*getWeight = 20 + rand.Intn(30) // 20-49
	remaining := total - *putWeight - *getWeight
	*deleteWeight = rand.Intn(min(remaining/2, 20) + 1)
	remaining -= *deleteWeight
	*batchWeight = rand.Intn(min(remaining/2, 15) + 1)
	remaining -= *batchWeight
	*iterWeight = rand.Intn(min(remaining/2, 10) + 1)
	*snapshotWeight = remaining - *iterWeight

	// Value size (50 to 500 bytes)
	*valueSize = 50 + rand.Intn(451)

	// Number of column families (1 to 4)
	*numColumnFamilies = 1 + rand.Intn(4)
}

const stressTestDirPrefix = "rockyard-stress-"

func setupDBPath() string {
	// Clean up old test directories if requested
	if *cleanup {
		cleanupOldTestDirs()
	}

	var testDir string
	var err error
	if *dbPath == "" {
		testDir, err = os.MkdirTemp("", stressTestDirPrefix+"*")
		if err != nil {
			fatal("Failed to create temp dir: %v", err)
		}
	} else {
		testDir = *dbPath
	}
	fmt.Printf("ğŸ“ Database path: %s\n\n", testDir)
	return testDir
}

// cleanupOldTestDirs removes old test directories from previous runs.
// This handles cleanup from crashed test processes where defer didn't run.
func cleanupOldTestDirs() {
	tempDir := os.TempDir()
	entries, err := os.ReadDir(tempDir)
	if err != nil {
		fmt.Printf("Warning: could not read temp dir for cleanup: %v\n", err)
		return
	}

	var cleaned int
	for _, entry := range entries {
		if !entry.IsDir() {
			continue
		}
		name := entry.Name()
		if len(name) >= len(stressTestDirPrefix) && name[:len(stressTestDirPrefix)] == stressTestDirPrefix {
			fullPath := filepath.Join(tempDir, name)
			if err := os.RemoveAll(fullPath); err != nil {
				fmt.Printf("Warning: could not remove %s: %v\n", fullPath, err)
			} else {
				cleaned++
			}
		}
	}

	if cleaned > 0 {
		fmt.Printf("ğŸ§¹ Cleaned up %d old test directories\n", cleaned)
	}
}

func printStats(stats *Stats) {
	fmt.Println()
	fmt.Println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println("                      FINAL STATISTICS                            ")
	fmt.Println("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

	totalOps := stats.puts.Load() + stats.gets.Load() + stats.deletes.Load() +
		stats.batches.Load() + stats.iterScans.Load() + stats.snapshotReads.Load() +
		stats.transactions.Load()

	fmt.Printf("Write Operations:\n")
	fmt.Printf("  Puts:        %12d\n", stats.puts.Load())
	fmt.Printf("  Deletes:     %12d\n", stats.deletes.Load())
	fmt.Printf("  Range Dels:  %12d\n", stats.rangeDeletes.Load())
	fmt.Printf("  Merges:      %12d\n", stats.merges.Load())
	fmt.Printf("  Batches:     %12d\n", stats.batches.Load())
	fmt.Printf("  Ingests:     %12d\n", stats.ingests.Load())

	fmt.Printf("\nTransactions:\n")
	fmt.Printf("  Total:       %12d\n", stats.transactions.Load())
	fmt.Printf("  Commits:     %12d\n", stats.txnCommits.Load())
	fmt.Printf("  Rollbacks:   %12d\n", stats.txnRollbacks.Load())

	fmt.Printf("\nColumn Families:\n")
	fmt.Printf("  CF Ops:      %12d\n", stats.cfOps.Load())

	fmt.Printf("\nRead Operations:\n")
	fmt.Printf("  Gets:        %12d\n", stats.gets.Load())
	fmt.Printf("  Iter Scans:  %12d\n", stats.iterScans.Load())
	fmt.Printf("  Snapshots:   %12d\n", stats.snapshotReads.Load())

	fmt.Printf("\nMaintenance:\n")
	fmt.Printf("  Reopens:     %12d\n", stats.reopens.Load())
	fmt.Printf("  Flushes:     %12d\n", stats.flushes.Load())
	fmt.Printf("  Compactions: %12d\n", stats.compactions.Load())

	fmt.Printf("\nVerification:\n")
	fmt.Printf("  Spot Checks: %12d passed\n", stats.spotVerifyPass.Load())
	fmt.Printf("  Snapshot:    %12d verified\n", stats.snapshotVerifies.Load())
	fmt.Printf("  Iterator:    %12d verified\n", stats.iterVerifies.Load())
	fmt.Printf("  Failures:    %12d\n", stats.verifyFail.Load())
	fmt.Printf("  Errors:      %12d\n", stats.errors.Load())

	fmt.Printf("\nTotal Operations: %d\n", totalOps)
	fmt.Println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
}

// dbHolder holds the current database instance with synchronization.
type dbHolder struct {
	mu             sync.RWMutex
	db             db.DB
	path           string
	opCount        atomic.Uint64
	lastCompact    atomic.Uint64
	columnFamilies []db.ColumnFamilyHandle // Additional column families (index 0 = cf1, etc.)
}

func runStressTest(dbPath string, expected *testutil.ExpectedStateV2, stats *Stats) error {
	// Open database
	database, cfs, err := openDB(dbPath)
	if err != nil {
		return fmt.Errorf("initial open failed: %w", err)
	}

	// Stop channels
	stop := make(chan struct{})
	var wg sync.WaitGroup

	// Database holder for safe access
	holder := &dbHolder{db: database, path: dbPath, columnFamilies: cfs}

	// Start worker threads
	for i := range *numThreads {
		wg.Go(func() {
			runWorker(i, holder, expected, stats, stop)
		})
	}

	// Start reopen thread if enabled
	if *reopenPeriod > 0 {
		wg.Go(func() {
			runReopener(holder, stats, stop)
		})
	}

	// Start flush thread if enabled
	if *flushPeriod > 0 {
		wg.Go(func() {
			runFlusher(holder, stats, stop)
		})
	}

	// Progress reporting
	startTime := time.Now()
	ticker := time.NewTicker(5 * time.Second)
	defer ticker.Stop()

	go func() {
		for {
			select {
			case <-stop:
				return
			case <-ticker.C:
				elapsed := time.Since(startTime)
				ops := stats.puts.Load() + stats.gets.Load() + stats.deletes.Load() +
					stats.batches.Load() + stats.iterScans.Load() + stats.snapshotReads.Load()
				opsPerSec := float64(ops) / elapsed.Seconds()

				var memStats runtime.MemStats
				runtime.ReadMemStats(&memStats)
				memMB := float64(memStats.Alloc) / 1024 / 1024

				fmt.Printf("â±ï¸  %v | %.0f ops/sec | %d errors | %.1f MB mem\n",
					elapsed.Round(time.Second), opsPerSec, stats.errors.Load(), memMB)
			}
		}
	}()

	// Run for duration
	time.Sleep(*duration)
	close(stop)
	wg.Wait()

	// Final verification
	fmt.Println("\nğŸ” Running final verification...")
	holder.mu.RLock()
	err = verifyAll(holder.db, expected, stats)
	holder.mu.RUnlock()
	if err != nil {
		return fmt.Errorf("final verification failed: %w", err)
	}

	// Close database
	holder.mu.Lock()
	holder.db.Close()
	holder.mu.Unlock()

	return nil
}

func openDB(path string) (db.DB, []db.ColumnFamilyHandle, error) {
	opts := db.DefaultOptions()
	opts.CreateIfMissing = true
	opts.WriteBufferSize = 4 * 1024 * 1024 // 4MB
	// Add a merge operator for stress testing
	opts.MergeOperator = &db.StringAppendOperator{Delimiter: ","}
	database, err := db.Open(filepath.Join(path, "db"), opts)
	if err != nil {
		return nil, nil, err
	}

	// Create additional column families if requested
	var cfs []db.ColumnFamilyHandle
	for i := 1; i < *numColumnFamilies; i++ {
		cfName := fmt.Sprintf("cf%d", i)

		// Check if CF already exists
		cf := database.GetColumnFamily(cfName)
		if cf == nil {
			// Create new CF
			cfOpts := db.DefaultColumnFamilyOptions()
			cf, err = database.CreateColumnFamily(cfOpts, cfName)
			if err != nil {
				// May already exist, try to get it
				cf = database.GetColumnFamily(cfName)
				if cf == nil {
					if *verbose {
						fmt.Printf("Warning: Could not create or get CF %s: %v\n", cfName, err)
					}
					continue
				}
			}
		}
		cfs = append(cfs, cf)
	}

	if *verbose && len(cfs) > 0 {
		fmt.Printf("ğŸ“Š Using %d column families\n", len(cfs)+1)
	}

	return database, cfs, nil
}

func runWorker(threadID int, holder *dbHolder, expected *testutil.ExpectedStateV2, stats *Stats, stop chan struct{}) {
	// Derive per-thread seed from the global seed for reproducibility.
	// This ensures that when crashtest passes -seed=N, all worker threads
	// produce deterministic behavior that can be reproduced.
	rng := rand.New(rand.NewSource(*seed + int64(threadID*1000)))
	totalWeight := *putWeight + *getWeight + *deleteWeight + *batchWeight + *iterWeight + *snapshotWeight +
		*rangeDelWeight + *mergeWeight + *ingestWeight + *transactionWeight + *compactWeight + *snapshotVerifyWeight + *cfWeight

	opsSinceVerify := 0

	isStopped := func() bool {
		select {
		case <-stop:
			return true
		default:
			return false
		}
	}

	for {
		if isStopped() {
			return
		}

		// Pick operation
		r := rng.Intn(totalWeight)

		// Try to acquire read lock with TryRLock + polling
		for {
			if isStopped() {
				return
			}
			if holder.mu.TryRLock() {
				break
			}
			time.Sleep(100 * time.Microsecond)
		}

		database := holder.db
		if database == nil {
			holder.mu.RUnlock()
			continue
		}

		var err error
		cumWeight := 0
		switch {
		case r < cumWeight+*putWeight:
			err = doPut(database, expected, stats, rng, stop)
		case r < cumWeight+*putWeight+*getWeight:
			err = doGet(database, expected, stats, rng)
		case r < cumWeight+*putWeight+*getWeight+*deleteWeight:
			err = doDelete(database, expected, stats, rng, stop)
		case r < cumWeight+*putWeight+*getWeight+*deleteWeight+*batchWeight:
			err = doBatch(database, expected, stats, rng, stop)
		case r < cumWeight+*putWeight+*getWeight+*deleteWeight+*batchWeight+*iterWeight:
			err = doIterScan(database, expected, stats, rng)
		case r < cumWeight+*putWeight+*getWeight+*deleteWeight+*batchWeight+*iterWeight+*snapshotWeight:
			err = doSnapshotRead(database, stats, rng)
		case r < cumWeight+*putWeight+*getWeight+*deleteWeight+*batchWeight+*iterWeight+*snapshotWeight+*rangeDelWeight:
			err = doRangeDelete(database, expected, stats, rng, stop)
		case r < cumWeight+*putWeight+*getWeight+*deleteWeight+*batchWeight+*iterWeight+*snapshotWeight+*rangeDelWeight+*mergeWeight:
			err = doMerge(database, expected, stats, rng, stop)
		case r < cumWeight+*putWeight+*getWeight+*deleteWeight+*batchWeight+*iterWeight+*snapshotWeight+*rangeDelWeight+*mergeWeight+*ingestWeight:
			err = doIngest(holder, expected, stats, rng, stop)
		case r < cumWeight+*putWeight+*getWeight+*deleteWeight+*batchWeight+*iterWeight+*snapshotWeight+*rangeDelWeight+*mergeWeight+*ingestWeight+*transactionWeight:
			err = doTransaction(holder, expected, stats, rng, stop)
		case r < cumWeight+*putWeight+*getWeight+*deleteWeight+*batchWeight+*iterWeight+*snapshotWeight+*rangeDelWeight+*mergeWeight+*ingestWeight+*transactionWeight+*compactWeight:
			err = doCompactAndVerify(database, expected, stats, rng)
		case r < cumWeight+*putWeight+*getWeight+*deleteWeight+*batchWeight+*iterWeight+*snapshotWeight+*rangeDelWeight+*mergeWeight+*ingestWeight+*transactionWeight+*compactWeight+*snapshotVerifyWeight:
			err = doSnapshotVerify(database, expected, stats, rng)
		default:
			err = doColumnFamilyOps(holder, stats, rng)
		}
		_ = cumWeight // silence unused variable warning

		holder.mu.RUnlock()

		if err != nil && !errors.Is(err, errStopped) {
			stats.errors.Add(1)
			if *verbose {
				fmt.Printf("Thread %d error: %v\n", threadID, err)
			}
		}

		// Periodic spot verification
		opsSinceVerify++
		if *verifyEvery > 0 && opsSinceVerify >= *verifyEvery && !isStopped() {
			opsSinceVerify = 0
			holder.mu.RLock()
			if holder.db != nil {
				doSpotVerify(holder.db, expected, stats, rng)
			}
			holder.mu.RUnlock()
		}

		// Track operations for compaction
		opCount := holder.opCount.Add(1)
		if *compactEvery > 0 && opCount-holder.lastCompact.Load() >= uint64(*compactEvery) && !isStopped() {
			holder.mu.RLock()
			if holder.db != nil {
				// Trigger compaction by flushing
				_ = holder.db.Flush(nil)
				stats.compactions.Add(1)
				holder.lastCompact.Store(opCount)
			}
			holder.mu.RUnlock()
		}
	}
}

// doPut performs a put operation WITH per-key locking (RocksDB-style)
func doPut(database db.DB, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand, stop chan struct{}) error {
	key := rng.Int63n(*numKeys)

	// ACQUIRE PER-KEY LOCK BEFORE OPERATION (matching C++ db_stress)
	mu := expected.GetMutexForKey(0, key)

	// Try to acquire lock with TryLock + polling for stop
	for {
		select {
		case <-stop:
			return errStopped
		default:
		}
		if mu.TryLock() {
			break
		}
		// Brief sleep to avoid busy-wait
		time.Sleep(100 * time.Microsecond)
	}
	defer mu.Unlock()

	// Prepare the expected value (sets pending write flag)
	pendingValue := expected.PreparePut(0, key)
	if pendingValue == nil {
		return fmt.Errorf("failed to prepare put for key %d", key)
	}

	// Get the value base that will be set after commit
	valueBase := pendingValue.GetFinalValueBase()

	keyBytes := makeKey(key)
	valueBytes := makeValue(key, valueBase)

	// Perform database operation
	if err := database.Put(stressWriteOpts, keyBytes, valueBytes); err != nil {
		// Rollback on failure
		pendingValue.Rollback()
		return fmt.Errorf("put failed: %w", err)
	}

	// Commit the expected state update
	pendingValue.Commit()
	stats.puts.Add(1)
	return nil
}

// doPutCF performs a put to a random column family.
// Note: Currently expected state only tracks CF 0 (default), so CF writes
// are not verified. This tests CF functionality without oracle verification.
// Reserved for future multi-CF stress testing.
func doPutCF(holder *dbHolder, stats *Stats, rng *rand.Rand) error { //nolint:unused // reserved for future use
	if len(holder.columnFamilies) == 0 {
		return nil // No additional CFs configured
	}

	// Pick a random CF (not default)
	cfIdx := rng.Intn(len(holder.columnFamilies))
	cf := holder.columnFamilies[cfIdx]

	key := rng.Int63n(*numKeys)
	keyBytes := makeKey(key)
	valueBytes := fmt.Appendf(nil, "cf%d_value_%d", cfIdx+1, rng.Int63())

	if err := holder.db.PutCF(nil, cf, keyBytes, valueBytes); err != nil {
		return fmt.Errorf("PutCF failed: %w", err)
	}

	return nil
}

// doColumnFamilyOps performs various operations on column families.
// This tests CF isolation, read/write, and iteration across CFs.
// IMPORTANT: CF operations use keys in a SEPARATE key space (cf_key prefix)
// to avoid interfering with the default CF keys tracked by expected state.
func doColumnFamilyOps(holder *dbHolder, stats *Stats, rng *rand.Rand) error {
	if len(holder.columnFamilies) == 0 {
		// No additional CFs, just do a regular operation
		return nil
	}

	// Pick operation type: 0=put, 1=get, 2=delete, 3=iterate, 4=verify isolation
	opType := rng.Intn(5)
	cfIdx := rng.Intn(len(holder.columnFamilies))
	cf := holder.columnFamilies[cfIdx]

	// Use a DIFFERENT key prefix for CF operations to avoid interfering with default CF
	key := rng.Int63n(*numKeys)
	keyBytes := fmt.Appendf(nil, "cf_key%016d", key)

	switch opType {
	case 0: // Put to CF
		valueBytes := fmt.Appendf(nil, "cf%d_v%d_%d", cfIdx+1, key, rng.Int63())
		if err := holder.db.PutCF(nil, cf, keyBytes, valueBytes); err != nil {
			return fmt.Errorf("PutCF failed: %w", err)
		}

	case 1: // Get from CF
		_, err := holder.db.GetCF(nil, cf, keyBytes)
		if err != nil && !errors.Is(err, db.ErrNotFound) {
			return fmt.Errorf("GetCF failed: %w", err)
		}

	case 2: // Delete from CF
		if err := holder.db.DeleteCF(nil, cf, keyBytes); err != nil {
			return fmt.Errorf("DeleteCF failed: %w", err)
		}

	case 3: // Iterate CF
		iter := holder.db.NewIteratorCF(nil, cf)
		if iter != nil {
			count := 0
			for iter.SeekToFirst(); iter.Valid() && count < 20; iter.Next() {
				_ = iter.Key()
				_ = iter.Value()
				count++
			}
			iter.Close()
		}

	case 4: // Verify CF isolation: write to one CF, verify not visible in another
		// Write a unique value to this CF
		uniqueKey := fmt.Appendf(nil, "cf_isolation_%d", rng.Int63())
		uniqueValue := fmt.Appendf(nil, "cf%d_unique", cfIdx+1)

		if err := holder.db.PutCF(nil, cf, uniqueKey, uniqueValue); err != nil {
			return fmt.Errorf("isolation test PutCF failed: %w", err)
		}

		// Verify it's in the CF we wrote to
		val, err := holder.db.GetCF(nil, cf, uniqueKey)
		if err != nil {
			return fmt.Errorf("isolation test GetCF own CF failed: %w", err)
		}
		if !bytes.Equal(val, uniqueValue) {
			return fmt.Errorf("isolation test: value mismatch in own CF")
		}

		// Verify it's NOT in the default CF
		_, err = holder.db.Get(nil, uniqueKey)
		if !errors.Is(err, db.ErrNotFound) {
			// This could happen if another thread wrote the same key to default CF
			// Not a hard error, just log
			if *verbose {
				fmt.Printf("CF isolation: key %s exists in default CF (may be concurrent write)\n", uniqueKey)
			}
		}

		// Clean up
		holder.db.DeleteCF(nil, cf, uniqueKey)
	}

	stats.cfOps.Add(1)
	return nil
}

// doGet performs a get operation WITH pre/post read verification (RocksDB-style)
func doGet(database db.DB, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand) error {
	key := rng.Int63n(*numKeys)
	keyBytes := makeKey(key)

	// Capture pre-read expected value (no lock needed for reads)
	preReadExpected := expected.Get(0, key)

	// Perform database operation
	value, err := database.Get(nil, keyBytes)

	// Capture post-read expected value
	postReadExpected := expected.Get(0, key)

	stats.gets.Add(1)

	// Verify using pre/post read pattern
	if errors.Is(err, db.ErrNotFound) {
		// Key not found - check if this is expected
		if testutil.MustHaveExisted(preReadExpected, postReadExpected) {
			// Key must have existed but we got NotFound - this is an error
			if *verbose {
				fmt.Printf("Get verification failed: key %d must have existed but got NotFound\n", key)
			}
			// Don't count as error during concurrent operations - concurrent deletes are possible
		}
		return nil
	} else if err != nil {
		return fmt.Errorf("get failed: %w", err)
	}

	// Key found - verify value is in expected range
	if testutil.MustHaveNotExisted(preReadExpected, postReadExpected) {
		// Key must NOT have existed but we found it - this is an error
		if *verbose {
			fmt.Printf("Get verification failed: key %d should not exist but found\n", key)
		}
		// Don't count as error - concurrent puts are possible
		return nil
	}

	// Verify value base is in expected range
	if len(value) >= 12 {
		valueBase := getValueBase(value)
		if !testutil.InExpectedValueBaseRange(valueBase, preReadExpected, postReadExpected) {
			if *verbose {
				fmt.Printf("Get verification: key %d value base %d out of expected range\n", key, valueBase)
			}
			// Don't count as hard error during concurrent operations
		}
	}

	return nil
}

// doDelete performs a delete operation WITH per-key locking (RocksDB-style)
func doDelete(database db.DB, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand, stop chan struct{}) error {
	key := rng.Int63n(*numKeys)

	// ACQUIRE PER-KEY LOCK BEFORE OPERATION
	mu := expected.GetMutexForKey(0, key)

	// Try to acquire lock with TryLock + polling for stop
	for {
		select {
		case <-stop:
			return errStopped
		default:
		}
		if mu.TryLock() {
			break
		}
		time.Sleep(100 * time.Microsecond)
	}
	defer mu.Unlock()

	// Prepare the expected value (sets pending delete flag)
	pendingValue := expected.PrepareDelete(0, key)
	if pendingValue == nil {
		return fmt.Errorf("failed to prepare delete for key %d", key)
	}

	keyBytes := makeKey(key)

	// Perform database operation
	if err := database.Delete(stressWriteOpts, keyBytes); err != nil {
		// Rollback on failure
		pendingValue.Rollback()
		return fmt.Errorf("delete failed: %w", err)
	}

	// Commit the expected state update
	pendingValue.Commit()
	stats.deletes.Add(1)
	return nil
}

// doBatch performs a batch write WITH per-key locking for all keys in the batch
func doBatch(database db.DB, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand, stop chan struct{}) error {
	wb := batch.New()
	batchSize := rng.Intn(20) + 1

	type opInfo struct {
		key          int64
		valueBase    uint32
		isDel        bool
		pendingValue *testutil.PendingExpectedValueV2
		mu           *sync.Mutex
	}
	ops := make([]opInfo, 0, batchSize)

	// Helper to check if stopped
	isStopped := func() bool {
		select {
		case <-stop:
			return true
		default:
			return false
		}
	}

	// Acquire locks and prepare all operations
	for range batchSize {
		if isStopped() {
			// Release any locks we've acquired
			for _, op := range ops {
				op.pendingValue.Rollback()
				op.mu.Unlock()
			}
			return errStopped
		}

		key := rng.Int63n(*numKeys)

		// Check if we already have this key in the batch (avoid double-locking)
		found := false
		for _, op := range ops {
			if op.key == key {
				found = true
				break
			}
		}
		if found {
			continue // Skip duplicate keys in same batch
		}

		mu := expected.GetMutexForKey(0, key)

		// Try to acquire lock with TryLock + polling for stop
		lockAcquired := false
		for !lockAcquired {
			if isStopped() {
				// Release any locks we've acquired
				for _, op := range ops {
					op.pendingValue.Rollback()
					op.mu.Unlock()
				}
				return errStopped
			}
			if mu.TryLock() {
				lockAcquired = true
			} else {
				time.Sleep(100 * time.Microsecond)
			}
		}

		if rng.Intn(10) < 2 {
			// 20% chance delete
			pendingValue := expected.PrepareDelete(0, key)
			wb.Delete(makeKey(key))
			ops = append(ops, opInfo{key: key, isDel: true, pendingValue: pendingValue, mu: mu})
		} else {
			pendingValue := expected.PreparePut(0, key)
			valueBase := pendingValue.GetFinalValueBase()
			wb.Put(makeKey(key), makeValue(key, valueBase))
			ops = append(ops, opInfo{key: key, valueBase: valueBase, isDel: false, pendingValue: pendingValue, mu: mu})
		}
	}

	// Perform batch write
	err := database.Write(stressWriteOpts, wb)

	// Commit or rollback all pending values and release locks
	for _, op := range ops {
		if err != nil {
			op.pendingValue.Rollback()
		} else {
			op.pendingValue.Commit()
		}
		op.mu.Unlock()
	}

	if err != nil {
		return fmt.Errorf("batch write failed: %w", err)
	}

	stats.batches.Add(1)
	return nil
}

func doIterScan(database db.DB, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand) error {
	// Create iterator
	iter := database.NewIterator(nil)
	if iter == nil {
		return fmt.Errorf("failed to create iterator")
	}
	defer iter.Close()

	// Random scan type
	scanType := rng.Intn(5)
	count := 0
	maxScan := 100 // Limit scan length

	switch scanType {
	case 0: // Forward from beginning with ordering verification
		var prevKey []byte
		for iter.SeekToFirst(); iter.Valid() && count < maxScan; iter.Next() {
			key := iter.Key()
			if prevKey != nil {
				if bytes.Compare(prevKey, key) >= 0 {
					stats.verifyFail.Add(1)
					return fmt.Errorf("iterator ordering violation: %q >= %q", prevKey, key)
				}
			}
			prevKey = append([]byte(nil), key...)
			count++
		}
		stats.iterVerifies.Add(1)
	case 1: // Backward from end with ordering verification
		var prevKey []byte
		for iter.SeekToLast(); iter.Valid() && count < maxScan; iter.Prev() {
			key := iter.Key()
			if prevKey != nil {
				if bytes.Compare(prevKey, key) <= 0 {
					stats.verifyFail.Add(1)
					return fmt.Errorf("reverse iterator ordering violation: %q <= %q", prevKey, key)
				}
			}
			prevKey = append([]byte(nil), key...)
			count++
		}
		stats.iterVerifies.Add(1)
	case 2: // Forward from random key with ordering verification
		startKey := rng.Int63n(*numKeys)
		iter.Seek(makeKey(startKey))
		var prevKey []byte
		for iter.Valid() && count < maxScan {
			key := iter.Key()
			if prevKey != nil {
				if bytes.Compare(prevKey, key) >= 0 {
					stats.verifyFail.Add(1)
					return fmt.Errorf("seek iterator ordering violation: %q >= %q", prevKey, key)
				}
			}
			prevKey = append([]byte(nil), key...)
			iter.Next()
			count++
		}
		stats.iterVerifies.Add(1)
	case 3: // Count all keys
		for iter.SeekToFirst(); iter.Valid(); iter.Next() {
			count++
			if count > int(*numKeys)+1000 {
				break // Safety limit
			}
		}
	case 4: // Verify values match expected state (spot check)
		startKey := rng.Int63n(*numKeys)
		iter.Seek(makeKey(startKey))
		verified := 0
		for iter.Valid() && verified < 10 {
			key := iter.Key()
			value := iter.Value()

			// Parse key to get key index
			if len(key) > 3 && string(key[:3]) == "key" {
				var keyIdx int64
				fmt.Sscanf(string(key), "key%d", &keyIdx)
				if keyIdx >= 0 && keyIdx < *numKeys {
					// Get expected value
					ev := expected.Get(0, keyIdx)
					if ev.Exists() && len(value) >= 12 {
						actualValueBase := getValueBase(value)
						expectedValueBase := ev.GetValueBase()
						if actualValueBase != expectedValueBase {
							// Don't count as hard failure during concurrent operations
							if *verbose {
								fmt.Printf("Iterator value mismatch: key %d got base %d, want %d\n",
									keyIdx, actualValueBase, expectedValueBase)
							}
						}
					}
					verified++
				}
			}
			iter.Next()
			count++
		}
	}

	if err := iter.Error(); err != nil {
		return fmt.Errorf("iterator error: %w", err)
	}

	stats.iterScans.Add(1)
	return nil
}

func doSnapshotRead(database db.DB, stats *Stats, rng *rand.Rand) error {
	// Get a snapshot
	snap := database.GetSnapshot()
	if snap == nil {
		return nil // Snapshots may not be fully implemented
	}
	defer database.ReleaseSnapshot(snap)

	// Read some keys from snapshot
	numReads := rng.Intn(10) + 1
	for range numReads {
		key := rng.Int63n(*numKeys)
		keyBytes := makeKey(key)

		// TODO: When snapshot reads are implemented, use snap parameter
		_, err := database.Get(nil, keyBytes)
		if err != nil && !errors.Is(err, db.ErrNotFound) {
			return fmt.Errorf("snapshot get failed: %w", err)
		}
	}

	stats.snapshotReads.Add(1)
	return nil
}

// doRangeDelete performs a range deletion WITH per-key locking for all keys in range.
// This is a simplified implementation that deletes a small range of keys.
func doRangeDelete(database db.DB, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand, stop chan struct{}) error {
	// Pick a random start key and range size (keep range small to limit lock contention)
	rangeSize := int64(rng.Intn(10) + 1) // 1-10 keys
	startKey := rng.Int63n(*numKeys - rangeSize)
	endKey := startKey + rangeSize

	// Helper to check if stopped
	isStopped := func() bool {
		select {
		case <-stop:
			return true
		default:
			return false
		}
	}

	// Acquire locks for all keys in range (in order to avoid deadlock)
	type lockInfo struct {
		key          int64
		mu           *sync.Mutex
		pendingValue *testutil.PendingExpectedValueV2
	}
	locks := make([]lockInfo, 0, rangeSize)

	// Acquire locks in order
	for key := startKey; key < endKey; key++ {
		if isStopped() {
			// Release any locks we've acquired
			for _, l := range locks {
				if l.pendingValue != nil {
					l.pendingValue.Rollback()
				}
				l.mu.Unlock()
			}
			return errStopped
		}

		mu := expected.GetMutexForKey(0, key)

		// Try to acquire lock with polling for stop
		for {
			if isStopped() {
				// Release any locks we've acquired
				for _, l := range locks {
					if l.pendingValue != nil {
						l.pendingValue.Rollback()
					}
					l.mu.Unlock()
				}
				return errStopped
			}
			if mu.TryLock() {
				break
			}
			time.Sleep(100 * time.Microsecond)
		}

		// Prepare delete for this key
		pendingValue := expected.PrepareDelete(0, key)
		locks = append(locks, lockInfo{key: key, mu: mu, pendingValue: pendingValue})
	}

	// Perform the range delete
	startKeyBytes := makeKey(startKey)
	endKeyBytes := makeKey(endKey)
	err := database.DeleteRange(nil, startKeyBytes, endKeyBytes)

	// Commit or rollback all pending values and release locks
	for _, l := range locks {
		if err != nil {
			l.pendingValue.Rollback()
		} else {
			l.pendingValue.Commit()
		}
		l.mu.Unlock()
	}

	if err != nil {
		return fmt.Errorf("range delete failed: %w", err)
	}

	stats.rangeDeletes.Add(1)
	return nil
}

// doMerge performs a merge operation.
// NOTE: Merge operations use a separate key prefix ("merge_key") to avoid interfering
// with the expected state oracle. The oracle tracks regular keys ("key%016d") and
// merge operations would corrupt those values. Using separate keys allows us to
// exercise the merge code path without affecting verification.
// See smoke tests for merge correctness verification.
func doMerge(database db.DB, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand, stop chan struct{}) error {
	_ = expected // Not tracked in expected state

	// Use a different key prefix to avoid corrupting tracked keys
	key := rng.Int63n(*numKeys)
	keyBytes := fmt.Appendf(nil, "merge_key%016d", key)

	// Create a small merge operand (using string append format)
	operand := fmt.Appendf(nil, "m%d", rng.Intn(1000))

	// Perform merge operation
	if err := database.Merge(stressWriteOpts, keyBytes, operand); err != nil {
		// Merge may fail if merge operator not set - not a real error for stress test
		if err.Error() == "db: merge operator not set in options" {
			return nil
		}
		return fmt.Errorf("merge failed: %w", err)
	}

	stats.merges.Add(1)
	return nil
}

// doIngest creates a small SST file and ingests it.
// This is an expensive operation, so it should have low weight.
func doIngest(holder *dbHolder, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand, stop chan struct{}) error {
	// Pick a range of keys to ingest (small range, 5-20 keys)
	numKeysToIngest := rng.Intn(16) + 5
	startKey := rng.Int63n(*numKeys - int64(numKeysToIngest))

	// Helper to check if stopped
	isStopped := func() bool {
		select {
		case <-stop:
			return true
		default:
			return false
		}
	}

	// Acquire locks for all keys we'll ingest (in order)
	type lockInfo struct {
		key          int64
		mu           *sync.Mutex
		pendingValue *testutil.PendingExpectedValueV2
		valueBase    uint32
	}
	locks := make([]lockInfo, 0, numKeysToIngest)

	// Track which lock buckets we've already locked to avoid self-deadlock
	lockedMutexes := make(map[*sync.Mutex]bool)

	for i := range numKeysToIngest {
		key := startKey + int64(i)

		if isStopped() {
			for _, l := range locks {
				if l.pendingValue != nil {
					l.pendingValue.Rollback()
				}
			}
			for mu := range lockedMutexes {
				mu.Unlock()
			}
			return errStopped
		}

		mu := expected.GetMutexForKey(0, key)

		// Only lock if we haven't already locked this mutex (multiple keys per lock)
		if !lockedMutexes[mu] {
			for {
				if isStopped() {
					for _, l := range locks {
						if l.pendingValue != nil {
							l.pendingValue.Rollback()
						}
					}
					for m := range lockedMutexes {
						m.Unlock()
					}
					return errStopped
				}
				if mu.TryLock() {
					lockedMutexes[mu] = true
					break
				}
				time.Sleep(100 * time.Microsecond)
			}
		}

		pendingValue := expected.PreparePut(0, key)
		valueBase := pendingValue.GetFinalValueBase()
		locks = append(locks, lockInfo{key: key, mu: mu, pendingValue: pendingValue, valueBase: valueBase})
	}

	// Helper to cleanup on error
	cleanup := func(rollback bool) {
		for _, l := range locks {
			if rollback && l.pendingValue != nil {
				l.pendingValue.Rollback()
			}
		}
		for mu := range lockedMutexes {
			mu.Unlock()
		}
	}

	// Create temp SST file
	sstPath := filepath.Join(holder.path, fmt.Sprintf("ingest_%d_%d.sst", time.Now().UnixNano(), rng.Int63()))

	writerOpts := db.DefaultSstFileWriterOptions()
	writer := db.NewSstFileWriter(writerOpts)

	if err := writer.Open(sstPath); err != nil {
		cleanup(true)
		return fmt.Errorf("failed to open SST writer: %w", err)
	}

	// Write keys to SST (keys must be sorted)
	for _, l := range locks {
		keyBytes := makeKey(l.key)
		valueBytes := makeValue(l.key, l.valueBase)
		if err := writer.Put(keyBytes, valueBytes); err != nil {
			writer.Abandon()
			os.Remove(sstPath)
			cleanup(true)
			return fmt.Errorf("SST write failed: %w", err)
		}
	}

	if _, err := writer.Finish(); err != nil {
		os.Remove(sstPath)
		cleanup(true)
		return fmt.Errorf("SST finish failed: %w", err)
	}

	// Ingest the SST file
	ingestOpts := db.IngestExternalFileOptions{
		MoveFiles:           true, // Move (delete) the file after ingest
		SnapshotConsistency: true,
		AllowGlobalSeqNo:    true,
		AllowBlockingFlush:  true,
	}

	err := holder.db.IngestExternalFile([]string{sstPath}, ingestOpts)

	// Commit or rollback
	if err != nil {
		for _, l := range locks {
			l.pendingValue.Rollback()
		}
	} else {
		for _, l := range locks {
			l.pendingValue.Commit()
		}
	}

	// Unlock all mutexes
	for mu := range lockedMutexes {
		mu.Unlock()
	}

	// Clean up SST file if it wasn't moved
	os.Remove(sstPath)

	if err != nil {
		return fmt.Errorf("ingest failed: %w", err)
	}

	stats.ingests.Add(1)
	return nil
}

// doTransaction performs a mini-transaction with puts and gets.
// Since we're using the regular DB (not TransactionDB) in the stress test,
// this simulates transaction-like behavior using batches with verification.
func doTransaction(holder *dbHolder, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand, stop chan struct{}) error {
	// Pick a small set of keys for the "transaction"
	numOps := rng.Intn(5) + 2 // 2-6 operations

	// Helper to check if stopped
	isStopped := func() bool {
		select {
		case <-stop:
			return true
		default:
			return false
		}
	}

	// Acquire locks for all keys (in order to avoid deadlock)
	type lockInfo struct {
		key          int64
		mu           *sync.Mutex
		pendingValue *testutil.PendingExpectedValueV2
		valueBase    uint32
		isPut        bool
	}
	locks := make([]lockInfo, 0, numOps)
	lockedMutexes := make(map[*sync.Mutex]bool)

	// Decide upfront whether to commit or rollback (80% commit)
	willCommit := rng.Intn(10) < 8

	lockedKeys := make(map[int64]bool)

	for range numOps {
		key := rng.Int63n(*numKeys)

		// Skip duplicate keys in same transaction
		if lockedKeys[key] {
			continue
		}

		if isStopped() {
			for _, l := range locks {
				if l.pendingValue != nil {
					l.pendingValue.Rollback()
				}
			}
			for mu := range lockedMutexes {
				mu.Unlock()
			}
			return errStopped
		}

		mu := expected.GetMutexForKey(0, key)

		// Only lock if we haven't already (same mutex may cover multiple keys)
		if !lockedMutexes[mu] {
			for {
				if isStopped() {
					for _, l := range locks {
						if l.pendingValue != nil {
							l.pendingValue.Rollback()
						}
					}
					for m := range lockedMutexes {
						m.Unlock()
					}
					return errStopped
				}
				if mu.TryLock() {
					lockedMutexes[mu] = true
					break
				}
				time.Sleep(100 * time.Microsecond)
			}
		}

		// 80% puts, 20% deletes
		if rng.Intn(10) < 8 {
			pendingValue := expected.PreparePut(0, key)
			valueBase := pendingValue.GetFinalValueBase()
			locks = append(locks, lockInfo{key: key, mu: mu, pendingValue: pendingValue, valueBase: valueBase, isPut: true})
		} else {
			pendingValue := expected.PrepareDelete(0, key)
			locks = append(locks, lockInfo{key: key, mu: mu, pendingValue: pendingValue, isPut: false})
		}
		lockedKeys[key] = true
	}

	// Build the batch
	wb := batch.New()
	for _, l := range locks {
		if l.isPut {
			wb.Put(makeKey(l.key), makeValue(l.key, l.valueBase))
		} else {
			wb.Delete(makeKey(l.key))
		}
	}

	// Execute or "rollback"
	var execErr error
	if willCommit {
		execErr = holder.db.Write(stressWriteOpts, wb)
	}

	// Commit or rollback expected state
	for _, l := range locks {
		if willCommit && execErr == nil {
			l.pendingValue.Commit()
		} else {
			l.pendingValue.Rollback()
		}
	}

	// Unlock all
	for mu := range lockedMutexes {
		mu.Unlock()
	}

	stats.transactions.Add(1)
	if willCommit && execErr == nil {
		stats.txnCommits.Add(1)
	} else {
		stats.txnRollbacks.Add(1)
	}

	if execErr != nil {
		return fmt.Errorf("transaction write failed: %w", execErr)
	}

	return nil
}

// doCompactAndVerify triggers a compaction and verifies a sample of keys afterward.
// This helps catch data corruption during compaction.
func doCompactAndVerify(database db.DB, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand) error {
	// Flush first to ensure data is in SST files
	// Note: "immutable memtable already exists" is a transient condition under concurrent load
	if err := database.Flush(nil); err != nil {
		if strings.Contains(err.Error(), "immutable memtable already exists") {
			// Transient condition - flush is in progress, skip and continue
			if *verbose {
				fmt.Printf("Flush skipped (memtable busy)\n")
			}
		} else {
			return fmt.Errorf("flush failed: %w", err)
		}
	}

	// Trigger compaction
	if err := database.CompactRange(nil, nil, nil); err != nil {
		// CompactRange may not be fully implemented - that's okay
		if *verbose {
			fmt.Printf("CompactRange: %v (may not be implemented)\n", err)
		}
	}

	stats.compactions.Add(1)

	// Verify a sample of keys after compaction with per-key locking
	// (required for accurate verification during concurrent stress operations)
	numToVerify := min(50, int(*numKeys))

	failures := 0
	for range numToVerify {
		key := rng.Int63n(*numKeys)
		keyBytes := makeKey(key)

		// Acquire per-key lock for accurate verification
		mu := expected.GetMutexForKey(0, key)
		mu.Lock()

		ev := expected.Get(0, key)
		value, err := database.Get(nil, keyBytes)

		// Skip keys with pending operations - they're in flux
		if ev.PendingWrite() || ev.PendingDelete() {
			mu.Unlock()
			continue
		}

		if ev.IsDeleted() {
			// Key should not exist
			if !errors.Is(err, db.ErrNotFound) {
				failures++
				if *verbose {
					fmt.Printf("Post-compaction verify: key %d exists but expected deleted\n", key)
				}
			}
		} else if ev.Exists() {
			// Key should exist
			if errors.Is(err, db.ErrNotFound) {
				failures++
				if *verbose {
					fmt.Printf("Post-compaction verify: key %d missing but expected to exist\n", key)
				}
			} else if err != nil {
				failures++
			} else if len(value) >= 12 {
				expectedValueBase := ev.GetValueBase()
				actualValueBase := getValueBase(value)
				if actualValueBase != expectedValueBase {
					failures++
					if *verbose {
						fmt.Printf("Post-compaction verify: key %d value base %d != expected %d\n",
							key, actualValueBase, expectedValueBase)
					}
				}
			}
		}
		// Skip keys in unknown state (neither deleted nor definitely existing)

		mu.Unlock()
	}

	if failures > 0 {
		// Post-compaction verification is best-effort during high concurrency.
		// Under heavy contention, transient races between expected state updates
		// and database reads can cause spurious failures. We log these but don't
		// treat them as fatal - final verification is the authoritative check.
		if *verbose {
			fmt.Printf("Post-compaction verify: %d potential inconsistencies (non-fatal)\n", failures)
		}
	}

	return nil
}

// doSnapshotVerify takes a snapshot and verifies it provides isolation.
// This is a read-only verification that doesn't modify DB state to avoid
// corrupting the expected state oracle.
func doSnapshotVerify(database db.DB, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand) error {
	// Take snapshot
	snap := database.GetSnapshot()
	if snap == nil {
		// Snapshots not implemented
		return nil
	}
	defer database.ReleaseSnapshot(snap)

	// Pick some keys to verify - just do read-only verification
	numToVerify := 10
	readOpts := db.DefaultReadOptions()
	readOpts.Snapshot = snap

	for range numToVerify {
		key := rng.Int63n(*numKeys)
		keyBytes := makeKey(key)

		// Pre-read expected value
		preRead := expected.Get(0, key)

		// Read from snapshot
		value, err := database.Get(readOpts, keyBytes)

		// Post-read expected value
		postRead := expected.Get(0, key)

		// Verify using pre/post pattern (read-only)
		if errors.Is(err, db.ErrNotFound) {
			// Key not found in snapshot
			if testutil.MustHaveExisted(preRead, postRead) {
				if *verbose {
					fmt.Printf("Snapshot verify: key %d not in snapshot but expected to exist\n", key)
				}
			}
		} else if err != nil {
			return fmt.Errorf("snapshot get failed: %w", err)
		} else {
			// Key found in snapshot - verify value is consistent
			if len(value) >= 12 {
				valueBase := getValueBase(value)
				if !testutil.InExpectedValueBaseRange(valueBase, preRead, postRead) {
					if *verbose {
						fmt.Printf("Snapshot verify: key %d value base %d out of range\n", key, valueBase)
					}
				}
			}
		}
	}

	stats.snapshotVerifies.Add(1)
	return nil
}

// doSpotVerify performs spot verification using pre/post read pattern
func doSpotVerify(database db.DB, expected *testutil.ExpectedStateV2, stats *Stats, rng *rand.Rand) {
	// Verify a random subset of keys
	numToVerify := min(max(int(int64(*verifyPercent)**numKeys/100), 1), 100)

	failures := 0
	for range numToVerify {
		key := rng.Int63n(*numKeys)

		// Pre-read expected value
		preRead := expected.Get(0, key)

		keyBytes := makeKey(key)
		value, err := database.Get(nil, keyBytes)

		// Post-read expected value
		postRead := expected.Get(0, key)

		// Verify using pre/post pattern
		if errors.Is(err, db.ErrNotFound) {
			// Key not found - check if this is expected
			if testutil.MustHaveExisted(preRead, postRead) {
				failures++
			}
		} else if err != nil {
			failures++ // Unexpected error
		} else {
			// Key found
			if testutil.MustHaveNotExisted(preRead, postRead) {
				failures++
			} else if len(value) >= 12 {
				valueBase := getValueBase(value)
				if !testutil.InExpectedValueBaseRange(valueBase, preRead, postRead) {
					failures++
				}
			}
		}
	}

	if failures == 0 {
		stats.spotVerifyPass.Add(1)
	} else {
		stats.verifyFail.Add(uint64(failures))
		if *verbose {
			fmt.Printf("âš ï¸  Spot verify: %d failures\n", failures)
		}
	}
}

func runReopener(holder *dbHolder, stats *Stats, stop chan struct{}) {
	ticker := time.NewTicker(*reopenPeriod)
	defer ticker.Stop()

	for {
		select {
		case <-stop:
			return
		case <-ticker.C:
			holder.mu.Lock()
			if *verbose {
				fmt.Println("ğŸ”„ Reopening database...")
			}

			// Close current
			if holder.db != nil {
				holder.db.Close()
			}

			// Reopen
			newDB, cfs, err := openDB(holder.path)
			if err != nil {
				fmt.Printf("âŒ Reopen failed: %v\n", err)
				holder.mu.Unlock()
				continue
			}

			holder.db = newDB
			holder.columnFamilies = cfs
			stats.reopens.Add(1)
			holder.mu.Unlock()

			if *verbose {
				fmt.Println("âœ… Database reopened")
			}
		}
	}
}

func runFlusher(holder *dbHolder, stats *Stats, stop chan struct{}) {
	ticker := time.NewTicker(*flushPeriod)
	defer ticker.Stop()

	for {
		select {
		case <-stop:
			return
		case <-ticker.C:
			holder.mu.RLock()
			if holder.db != nil {
				if err := holder.db.Flush(nil); err != nil {
					if *verbose {
						fmt.Printf("Flush error: %v\n", err)
					}
				} else {
					stats.flushes.Add(1)
				}
			}
			holder.mu.RUnlock()
		}
	}
}

// verifyAll performs final verification with per-key locking
func verifyAll(database db.DB, expected *testutil.ExpectedStateV2, stats *Stats) error {
	verified := 0
	failures := 0

	// Count keys that should exist based on expected state
	expectedExists := 0
	for key := range int64(*numKeys) {
		ev := expected.Get(0, key)
		if ev.Exists() {
			expectedExists++
		}
	}
	fmt.Printf("  Expected keys that should exist: %d\n", expectedExists)

	// Verify expected state matches database
	for key := range int64(*numKeys) {
		// Acquire lock for this key during verification
		mu := expected.GetMutexForKey(0, key)
		mu.Lock()

		ev := expected.Get(0, key)

		// Skip keys with pending operations - they shouldn't exist after test completion
		// but we need to handle any edge cases gracefully
		if ev.PendingWrite() || ev.PendingDelete() {
			mu.Unlock()
			continue
		}

		keyBytes := makeKey(key)
		value, err := database.Get(nil, keyBytes)

		if ev.IsDeleted() {
			// Key should not exist
			if !errors.Is(err, db.ErrNotFound) {
				if *allowDBAhead {
					// In crash testing, DB can be ahead of expected state due to race conditions.
					// Expected state: key deleted (saved before new PUT)
					// DB: key exists (PUT synced after expected state save, before SIGKILL)
					// This is acceptable - no data loss occurred.
					if *verbose {
						fmt.Printf("Verify: key %d expected deleted but found (allowed, DB ahead) (err=%v)\n", key, err)
					}
				} else {
					failures++
					if *verbose {
						fmt.Printf("Verify: key %d expected deleted but found (err=%v)\n", key, err)
					}
				}
			}
		} else if ev.Exists() {
			// Key should exist
			if err != nil {
				failures++
				if *verbose {
					fmt.Printf("Verify: key %d expected to exist but got error: %v\n", key, err)
				}
			} else {
				// Verify value base
				expectedValueBase := ev.GetValueBase()
				if len(value) >= 12 {
					actualValueBase := getValueBase(value)
					if actualValueBase != expectedValueBase {
						if *allowDBAhead && actualValueBase > expectedValueBase {
							// DB has newer value - acceptable in crash testing
							// (PUT synced after expected state save, before SIGKILL)
							if *verbose {
								fmt.Printf("Verify: key %d value base mismatch: got %d, want %d (allowed, DB ahead)\n",
									key, actualValueBase, expectedValueBase)
							}
						} else {
							failures++
							if *verbose {
								fmt.Printf("Verify: key %d value base mismatch: got %d, want %d\n",
									key, actualValueBase, expectedValueBase)
							}
						}
					}
				}
			}
		}
		// Skip keys with pending flags or unknown state

		mu.Unlock()
		verified++
	}

	fmt.Printf("  Verified %d keys, %d failures\n", verified, failures)
	stats.verifyFail.Add(uint64(failures))

	if failures > 0 {
		return fmt.Errorf("%d verification failures", failures)
	}
	return nil
}

func makeKey(key int64) []byte {
	return fmt.Appendf(nil, "key%016d", key)
}

// makeValue creates a value that encodes the key and value base for verification.
// Format: [key:8 bytes][valueBase:4 bytes][padding...]
func makeValue(key int64, valueBase uint32) []byte {
	value := make([]byte, *valueSize)
	if *valueSize < 12 {
		return value
	}

	// Encode key and value base in little-endian format
	binary.LittleEndian.PutUint64(value[0:8], uint64(key))
	binary.LittleEndian.PutUint32(value[8:12], valueBase)

	// Fill rest with deterministic data
	for i := 12; i < *valueSize; i++ {
		value[i] = byte((int(key) + int(valueBase) + i) % 256)
	}

	return value
}

// getValueBase extracts the value base from a value.
func getValueBase(value []byte) uint32 {
	if len(value) < 12 {
		return 0
	}
	return binary.LittleEndian.Uint32(value[8:12])
}

// verifyValue checks if a value matches the expected key and value base.
// Reserved for future value verification enhancements.
func verifyValue(key int64, expectedValueBase uint32, value []byte) bool { //nolint:unused // reserved for future use
	if len(value) < 12 {
		return false
	}
	storedKey := binary.LittleEndian.Uint64(value[0:8])
	storedBase := binary.LittleEndian.Uint32(value[8:12])
	return storedKey == uint64(key) && storedBase == expectedValueBase
}

func fatal(format string, args ...any) {
	fmt.Printf("FATAL: "+format+"\n", args...)
	os.Exit(1)
}
